{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "from collections import defaultdict, Counter\n",
    "import math\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import copy\n",
    "import random\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# must have n lines\n",
    "# read in contents of single file as dictionary\n",
    "# load all files as dict of dicts\n",
    "# calculate tf-idf for each word\n",
    "# get top n \n",
    "\n",
    "#sparse matrix representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to extract top N for specific sub\n",
    "## Function to get idf for specific word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in files to dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on: CrowdfundedBoardgames\n",
      "Time elapsed: 104.78139781951904 seconds\n",
      "Total Time elapsed: 104.7824158668518 seconds\n",
      "We have 11427 Subreddits\n",
      "Found problems with 122 subreddits\n"
     ]
    }
   ],
   "source": [
    "src_dir = '/Users/choldawa/Documents/Projects/RedditCorpus/word_count/RC_2012-12'\n",
    "\n",
    "tick = time.time()\n",
    "\n",
    "d = {}\n",
    "problem_subs = []\n",
    "cnt = 0\n",
    "for file in os.listdir(src_dir):\n",
    "    path = src_dir+\"/\"+file\n",
    "    with open(path) as f:\n",
    "        filename = os.path.basename(file).split(\"_\")[2].split(\".\")[0]\n",
    "        clear_output(wait=True)\n",
    "        print(f\"working on: {filename}\")\n",
    "        tock = time.time()\n",
    "        print(f\"Time elapsed: {tock - tick} seconds\")\n",
    "        sub_d = {}\n",
    "        num_lines = sum(1 for line in open(path)) #must have at least 100 unique words\n",
    "        if num_lines > 100:\n",
    "            for line in f:\n",
    "                try:\n",
    "                    (val, key) = line.strip().split() #invert the order \n",
    "                    if not any(map(str.isdigit, key)) and int(val)>1: # each word must occur more than once\n",
    "                         if key not in stop_words: #check if stopword   \n",
    "                            sub_d[key] = int(val)\n",
    "                except:\n",
    "                    problem_subs.append(filename)\n",
    "            d[filename] = sub_d\n",
    "end = time.time()\n",
    "print(f\"Total Time elapsed: {end - tick} seconds\")\n",
    "print(f\"We have {len(d)} Subreddits\")\n",
    "print(f\"Found problems with {len(problem_subs)} subreddits\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['terrariums',\n",
       " 'oklahoma',\n",
       " 'hackerspaces',\n",
       " 'TheContinuum',\n",
       " 'GothicMetal',\n",
       " 'metalgearsolid',\n",
       " 'auckland',\n",
       " 'Mabinogi',\n",
       " 'smashbros',\n",
       " 'fightporn',\n",
       " 'crazystairs']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(d.keys())[:11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are some example common words in r/politics:\n",
      "[('people', 116175), ('would', 84858), ('dont', 77698), ('like', 70930), ('one', 56556)]\n"
     ]
    }
   ],
   "source": [
    "subreddit = 'politics'\n",
    "print(f\"Here are some example common words in r/{subreddit}:\")\n",
    "print(list(d[subreddit].items())[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_sub_count(word, month_dictionary):\n",
    "    '''\n",
    "    get the count subs in a given month that contain a gieven word\n",
    "    requires a word, and a dictionary for that month\n",
    "    '''\n",
    "    N = len(month_dictionary)\n",
    "    \n",
    "    cnt = 0\n",
    "    for s in month_dictionary:\n",
    "        if word in month_dictionary[s]: #count of subs where w appears\n",
    "            cnt += 1\n",
    "    return cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "113"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_word_sub_count('bitcoin', d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SKLEARN tf-idf approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "dv = DictVectorizer()\n",
    "#D = [{'foo': 1, 'bar': 2}, {'foo': 3, 'baz': 1}] #EXAMPLE FORMAT\n",
    "X = dv.fit_transform(list(d.values()))\n",
    "tv = TfidfTransformer()\n",
    "tfidf = tv.fit_transform(X)\n",
    "print(tfidf.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'staircases'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dv.get_feature_names()[535048]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf.toarray().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_k_tfidf(k, d, tfidf_matrix, subreddit):\n",
    "    '''\n",
    "    Get the top tfidf embeddings for a given subreddit\n",
    "    Takes: k (top k tfidf words), \n",
    "            month dictionary,\n",
    "            the tfidf matrix, \n",
    "            and target subreddit\n",
    "    '''\n",
    "    index = list(d.keys()).index(subreddit)\n",
    "    # Take the indices of the largest k elements from each row\n",
    "    top_k_inds = np.argsort(tfidf[index,:].toarray())[:, -1:-k - 1:-1]\n",
    "#     # Take the values at those indices\n",
    "#     top_k = np.take_along_axis(tfidf.toarray(), top_k_inds, axis=-1)\n",
    "#     top_k_pairs = np.stack((top_k_inds, top_k), axis=2)\n",
    "    \n",
    "    #return list of top words\n",
    "    top_tfidf_words = []\n",
    "    for ind in top_k_inds[0]:\n",
    "        top_tfidf_words.append(dv.get_feature_names()[ind])\n",
    "    return top_tfidf_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The top 5 words for r/crazystairs are:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['stairs', 'nsfw', 'xpost', 'staircase', 'staircases']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k, subreddit = 5, 'crazystairs'\n",
    "\n",
    "print(f'The top {k} words for r/{subreddit} are:')\n",
    "get_top_k_tfidf(k, d, tfidf, subreddit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot histogram of word counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"We have {len(df)} unique words and {len(d)} unique subreddits\")\n",
    "logD = {k:np.log10(v) for k, v in df.items()}\n",
    "plt.hist(logD.values(), bins = 25)\n",
    "plt.xlabel(\"Log10 num of subreddits\")\n",
    "plt.ylabel(\"Count of words\")\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting sorted dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_reddit = 'theoffice' # theoffice\n",
    "topN = 6\n",
    "x = list(dict(sorted(sub_dict[sub_reddit].items(), \n",
    "                           key=lambda item: item[1], reverse = True)\n",
    "                    [:topN]).keys())\n",
    "y = list(dict(sorted(sub_dict[sub_reddit].items(), \n",
    "                           key=lambda item: item[1], reverse = True)\n",
    "                    [:topN]).values())\n",
    "\n",
    "\n",
    "plt.bar(x,y, align='center')\n",
    "plt.xticks(rotation=70)\n",
    "plt.title(sub_reddit)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
