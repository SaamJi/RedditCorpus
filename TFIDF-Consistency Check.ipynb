{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF calculator for Reddit months\n",
    "\n",
    "This file calculates tf-idf embeddings for words in each subreddit in a given month (subreddits as documents). Subreddits with less than 100 distinct words are excluded as well as all stopwords. The final output is represented as a sparse matrix. \n",
    "\n",
    "This notebook also provides functionality for counting the number of subs containing a given word and tracking these values over time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "from collections import defaultdict, Counter\n",
    "import math\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import copy\n",
    "import random\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For example, here are some example common words in r/politics:\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'politics'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-0fec59daa809>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0msubreddit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'politics'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"For example, here are some example common words in r/{subreddit}:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdict_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msubreddit\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: 'politics'"
     ]
    }
   ],
   "source": [
    "date_list = ['2005-12',\n",
    "             '2006-12',\n",
    "             '2007-12',\n",
    "             '2008-12',\n",
    "             '2009-12',\n",
    "             '2010-12',\n",
    "             '2011-12',\n",
    "             '2012-12',\n",
    "             '2013-12',\n",
    "             '2014-12',\n",
    "             '2015-12',\n",
    "             '2016-12']\n",
    "dict_list = []\n",
    "for date in date_list:\n",
    "    print(f\"loading {data}...\")\n",
    "    with open(f'dict_{date}.pickle', 'rb') as data:\n",
    "        d = pickle.load(data)\n",
    "    dict_list.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For example, here are some example common words in r/politics:\n",
      "[('people', 12075), ('would', 10133), ('dont', 8886), ('like', 8712), ('deleted', 8626)]\n"
     ]
    }
   ],
   "source": [
    "subreddit = 'politics'\n",
    "print(f\"For example, here are some example common words in r/{subreddit}:\")\n",
    "print(list(dict_list[3][subreddit].items())[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load our two comparison years (2013 and 2014)\n",
    "d13 = dict_list[8]\n",
    "d14 = dict_list[9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get most common subreddits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_N_subs(N, month_dictionary):\n",
    "    data = {}\n",
    "    for sub in month_dictionary.items():\n",
    "            data[sub[0]] = sum(sub[1].values())\n",
    "    return sorted(data.items(), key=lambda x: x[1],\n",
    "                  reverse = True)[:N]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('AskReddit', 54074734), ('AdviceAnimals', 13637911), ('leagueoflegends', 10108980), ('funny', 9534077), ('pics', 7317629)]\n",
      "[('leagueoflegends', 9801783), ('worldnews', 9195902), ('DestinyTheGame', 9090857), ('news', 8844423), ('nfl', 8454921)]\n"
     ]
    }
   ],
   "source": [
    "print(get_top_N_subs(5,d13))\n",
    "print(get_top_N_subs(5,d14))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get tfi-idf transformer and matrix using sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def build_vectorizer_tfidf_matrix(d):\n",
    "    \"\"\"\n",
    "    Provide the word count dictionary\n",
    "    Will output two objects: \n",
    "        - a dictVecotrizer which will be used to access the column names (words)\n",
    "        - the tfidf transformed matrix (sparse matrix) to get tf-idf values\n",
    "    \"\"\"\n",
    "    print(\"Building tfidf...\")\n",
    "    dv = DictVectorizer()\n",
    "    #D = [{'foo': 1, 'bar': 2}, {'foo': 3, 'baz': 1}] #EXAMPLE FORMAT\n",
    "    X = dv.fit_transform(list(d.values()))\n",
    "    tv = TfidfTransformer()\n",
    "    tfidf = tv.fit_transform(X)\n",
    "    print(\"done\")\n",
    "    return dv, tfidf\n",
    "#     print(tfidf.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run vectorizer, this can take some time\n",
    "tfidf = build_vectorizer_tfidf_matrix(d)\n",
    "print(\"shape:\", tfidf[1].toarray().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_k_tfidf(k, d, tfidf, subreddit):\n",
    "    '''\n",
    "    Get the top tfidf embeddings for a given subreddit\n",
    "    Takes: k (top k tfidf words), \n",
    "            month dictionary,\n",
    "            the tfidf matrix, \n",
    "            and target subreddit\n",
    "    '''\n",
    "    index = list(d.keys()).index(subreddit)\n",
    "    # Take the indices of the largest k elements from each row\n",
    "    top_k_inds = np.argsort(tfidf[1][index,:].toarray())[:, -1:-k - 1:-1]\n",
    "#     # Take the values at those indices\n",
    "#     top_k = np.take_along_axis(tfidf.toarray(), top_k_inds, axis=-1)\n",
    "#     top_k_pairs = np.stack((top_k_inds, top_k), axis=2)\n",
    "    \n",
    "    #return list of top words\n",
    "    top_tfidf_words = []\n",
    "    for ind in top_k_inds[0]:\n",
    "        top_tfidf_words.append(tfidf[0].get_feature_names()[ind])\n",
    "    return top_tfidf_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k, subreddit = 5, 'leagueoflegends'\n",
    "\n",
    "print(f'The top {k} words for r/{subreddit} are:')\n",
    "get_top_k_tfidf(k, d, tfidf, subreddit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get count of subs containing given word over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get count of subs containing given word over time\n",
    "date_list = ['2008-12', '2010-12','2012-12', '2014-12']\n",
    "dict_list = []\n",
    "for date in date_list:\n",
    "    dict_list.append(build_word_count_dictionary(date))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = 'bitcoin'\n",
    "sub_count_list = []\n",
    "\n",
    "for d in dict_list:\n",
    "    sub_count_list.append(get_word_sub_count(word, d)/len(d))\n",
    "print(sub_count_list)\n",
    "plt.plot(date_list, sub_count_list)\n",
    "for x,y in zip(date_list,sub_count_list):\n",
    "\n",
    "    label = \"{:.3f}\".format(y)\n",
    "\n",
    "    plt.annotate(label, # this is the text\n",
    "                 (x,y), # these are the coordinates to position the label\n",
    "                 textcoords=\"offset points\", # how to position the text\n",
    "                 xytext=(0,1), # distance from text to points (x,y)\n",
    "                 ha='center') # horizontal alignment can be left, right or center\n",
    "plt.title(f\"Word: '{word}'\")\n",
    "plt.ylabel(\"Prop of Subs\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_count_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot histogram of word counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"We have {len(df)} unique words and {len(d)} unique subreddits\")\n",
    "logD = {k:np.log10(v) for k, v in df.items()}\n",
    "plt.hist(logD.values(), bins = 25)\n",
    "plt.xlabel(\"Log10 num of subreddits\")\n",
    "plt.ylabel(\"Count of words\")\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting sorted dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_reddit = 'theoffice' # theoffice\n",
    "topN = 6\n",
    "x = list(dict(sorted(sub_dict[sub_reddit].items(), \n",
    "                           key=lambda item: item[1], reverse = True)\n",
    "                    [:topN]).keys())\n",
    "y = list(dict(sorted(sub_dict[sub_reddit].items(), \n",
    "                           key=lambda item: item[1], reverse = True)\n",
    "                    [:topN]).values())\n",
    "\n",
    "\n",
    "plt.bar(x,y, align='center')\n",
    "plt.xticks(rotation=70)\n",
    "plt.title(sub_reddit)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
